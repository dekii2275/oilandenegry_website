import os
import time
import json
import hashlib
import atexit
import newspaper
import google.generativeai as genai
import concurrent.futures
from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException
from sqlalchemy.orm import Session
# üëá TH√äM or_ V√ÄO ƒê√ÇY ƒê·ªÇ T√åM KI·∫æM TH√îNG MINH
from sqlalchemy import desc, or_
from sqlalchemy.exc import IntegrityError
from datetime import datetime

from apscheduler.schedulers.background import BackgroundScheduler
# üëá TH√äM CronTrigger ƒê·ªÇ CH·ªàNH GI·ªú CH·∫†Y
from apscheduler.triggers.cron import CronTrigger
from newspaper import Config
from tenacity import retry, stop_after_attempt, wait_exponential
from app.core.database import get_db, SessionLocal
from app.models.news import News

router = APIRouter()

# --- 1. C·∫§U H√åNH GEMINI ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-2.0-flash')
else:
    model = None
    print("‚ö†Ô∏è C·∫¢NH B√ÅO: Ch∆∞a c·∫•u h√¨nh GEMINI_API_KEY. AI s·∫Ω kh√¥ng ho·∫°t ƒë·ªông.")

ALLOWED_CATEGORIES = [
    "Th·ªã tr∆∞·ªùng nƒÉng l∆∞·ª£ng", "ƒêi·ªán & H·∫° t·∫ßng", "D·∫ßu kh√≠", 
    "NƒÉng l∆∞·ª£ng t√°i t·∫°o", "C√¥ng ngh·ªá xanh", "Ch√≠nh s√°ch", "Tin t·ª©c chung"
]

SOURCES = [
    "https://petrotimes.vn/nang-luong-viet-nam.html",
    "https://nangluongvietnam.vn/",
    "https://vnexpress.net/tag/nang-luong-19597",
    "https://congthuong.vn/nang-luong-tai-nguyen-c31",
    "https://cafef.vn/hang-hoa-nguyen-lieu.chn"
]

# --- 2. H√ÄM X·ª¨ L√ù AI ---
@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=5))
def analyze_article_with_gemini(text, original_title):
    if not model: return None
    
    prompt = f"""
    B·∫°n l√† chuy√™n gia nƒÉng l∆∞·ª£ng. H√£y ph√¢n t√≠ch v√† vi·∫øt l·∫°i b√†i b√°o sau d∆∞·ªõi d·∫°ng Markdown chuy√™n nghi·ªáp.
    Y√™u c·∫ßu B·∫ÆT BU·ªòC:
    1. "summary": T√≥m t·∫Øt ng·∫Øn g·ªçn 3 c√¢u ch·ªß ch·ªët.
    2. "category": Ch·ªçn ch√≠nh x√°c 1 trong c√°c m·ª•c: {ALLOWED_CATEGORIES}.
    3. "tags": Chu·ªói 3-4 t·ª´ kh√≥a.
    4. "formatted_content": Vi·∫øt l·∫°i n·ªôi dung ch√≠nh b√†i b√°o. QUAN TR·ªåNG: Ph·∫£i chia th√†nh nhi·ªÅu ƒëo·∫°n vƒÉn ng·∫Øn (kho·∫£ng 3-4 c√¢u/ƒëo·∫°n). Gi·ªØa c√°c ƒëo·∫°n vƒÉn PH·∫¢I c√≥ 2 d·∫•u xu·ªëng d√≤ng (\n\n) ƒë·ªÉ t·∫°o kho·∫£ng c√°ch. S·ª≠ d·ª•ng in ƒë·∫≠m (**) cho c√°c s·ªë li·ªáu quan tr·ªçng.

    Ti√™u ƒë·ªÅ g·ªëc: {original_title}
    N·ªôi dung g·ªëc: {text[:5000]}
    """
    try:
        response = model.generate_content(prompt)
        json_str = response.text.strip()
        if json_str.startswith("```"):
            json_str = json_str.strip("`").replace("json\n", "").replace("json", "")
            
        data = json.loads(json_str)
        if data.get("category") not in ALLOWED_CATEGORIES: 
            data["category"] = "Tin t·ª©c chung"
        return data
    except Exception as e:
        print(f"   ‚ö†Ô∏è L·ªói Gemini AI: {e}")
        return None

# --- 3. H√ÄM WORKER ---
def process_single_article(article_url, source_name, config):
    db = SessionLocal()
    keywords = ['d·∫ßu', 'xƒÉng', 'ƒëi·ªán', 'nƒÉng l∆∞·ª£ng', 'kh√≠', 'gas', 'evn', 'pvn', 'pin', 'gi√°', 'th·∫ßu', 'solar', 'gi√≥']
    
    try:
        # Check tr√πng URL
        exists = db.query(News).filter(News.original_url == article_url).first()
        if exists: return 0

        # T·∫£i b√†i b√°o
        article = newspaper.Article(article_url, config=config)
        try:
            article.download()
            article.parse()
        except Exception: return 0

        if not article.text or len(article.text) < 100: return 0

        # L·ªçc t·ª´ kh√≥a
        if not any(k in article.title.lower() for k in keywords): return 0

        print(f"   ‚ö° AI Processing: {article.title[:30]}...")

        # G·ªçi AI
        ai_data = None
        try:
            ai_data = analyze_article_with_gemini(article.text, article.title)
        except Exception: pass

        # Chu·∫©n b·ªã d·ªØ li·ªáu
        if ai_data:
            summary = ai_data.get("summary", article.text[:200])
            content = ai_data.get("formatted_content", article.text)
            category = ai_data.get("category", "Tin t·ª©c chung")
            tags = ai_data.get("tags", "")
            if isinstance(tags, list): tags = ", ".join(tags)
        else:
            summary = article.text[:300] + "..."
            content = article.text
            category = "Tin t·ª©c chung"
            tags = "NƒÉng l∆∞·ª£ng"

        # L∆∞u DB
        url_hash = hashlib.md5(article_url.encode()).hexdigest()[:8]
        slug = f"tin-{int(time.time())}-{url_hash}"

        new_news = News(
            title=article.title, slug=slug, original_url=article.url,
            image_url=article.top_image, source=source_name,
            published_at=article.publish_date or datetime.now(),
            summary=summary, content=content, category=category,
            tags=tags, author="Z-Energy Bot", views=0, is_published=True
        )

        try:
            db.add(new_news)
            db.commit()
            print(f"   ‚úÖ SAVED: {article.title[:20]}...")
            return 1
        except IntegrityError:
            db.rollback()
            return 0
    except Exception: return 0
    finally: db.close()

# --- 4. H√ÄM MAIN ---
def run_crawler_process():
    print(f"\nüöÄ [CRAWLER] B·∫Øt ƒë·∫ßu l√∫c {datetime.now()}")
    
    config = Config()
    config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    config.request_timeout = 20
    config.fetch_images = False
    config.memoize_articles = False
    config.headers = {
        'User-Agent': config.browser_user_agent,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Referer': '[https://www.google.com/](https://www.google.com/)'
    }

    total_new = 0

    for url in SOURCES:
        try:
            domain = url.split('/')[2]
            try:
                paper = newspaper.build(url, config=config, memoize_articles=False)
            except: continue
            
            # üëá TƒÇNG GI·ªöI H·∫†N L√äN 8 B√ÄI ƒê·ªÇ L·∫§Y NHI·ªÄU TIN H∆†N
            valid_urls = []
            for article in paper.articles:
                if len(valid_urls) >= 8: break
                if len(article.url) > 25 and domain in article.url:
                    valid_urls.append(article.url)
            
            if not valid_urls: continue

            print(f"   üì° {domain}: T√¨m th·∫•y {len(valid_urls)} link ti·ªÅm nƒÉng.")

            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                futures = {executor.submit(process_single_article, u, domain, config): u for u in valid_urls}
                for future in concurrent.futures.as_completed(futures):
                    total_new += future.result()
            
        except Exception as e:
            print(f"‚ùå L·ªói ngu·ªìn {url}: {e}")

    print(f"üèÅ [CRAWLER] Ho√†n t·∫•t. T·ªïng b√†i m·ªõi: {total_new}\n")

# --- 5. SCHEDULER (ƒê√É S·ª¨A GI·ªú) ---
scheduler = BackgroundScheduler()

def start_scheduler():
    # üëá CH·∫†Y ƒê√öNG 09:00 V√Ä 21:00 H√ÄNG NG√ÄY
    trigger = CronTrigger(hour='9,21', minute='0', timezone='Asia/Ho_Chi_Minh')
    
    scheduler.add_job(run_crawler_process, trigger=trigger, id='crawl_news_job', replace_existing=True)
    scheduler.start()
    print("‚è∞ Scheduler: ƒê√£ ƒë·∫∑t l·ªãch ch·∫°y 09:00 & 21:00.")
    atexit.register(lambda: scheduler.shutdown())

@router.post("/crawl-now")
def trigger_crawl(background_tasks: BackgroundTasks):
    background_tasks.add_task(run_crawler_process)
    return {"message": "ƒêang ch·∫°y Crawler..."}

# --- 6. API L·∫§Y TIN (ƒê√É S·ª¨A SMART SEARCH) ---
@router.get("/")
def get_news_list(
    skip: int = 0, 
    limit: int = 20, 
    category: str = None, 
    db: Session = Depends(get_db)
):
    query = db.query(News).filter(News.is_published == True)
    
    if category and category not in ["ALL", "All", "T·∫•t c·∫£"]:
        # Logic t√¨m ki·∫øm th√¥ng minh: T√¨m trong Category HO·∫∂C Tags HO·∫∂C Title
        search_term = category
        if "nƒÉng l∆∞·ª£ng" in category.lower():
            search_term = "nƒÉng l∆∞·ª£ng"
        elif "d·∫ßu" in category.lower():
            search_term = "d·∫ßu"
        elif "ƒëi·ªán" in category.lower():
            search_term = "ƒëi·ªán"
            
        query = query.filter(
            or_(
                News.category == category,
                News.category.ilike(f"%{category}%"),
                News.tags.ilike(f"%{search_term}%"),
                News.title.ilike(f"%{search_term}%")
            )
        )
        
    return query.order_by(desc(News.published_at)).offset(skip).limit(limit).all()

@router.get("/{slug}")
def get_news_detail(slug: str, db: Session = Depends(get_db)):
    news = db.query(News).filter(News.slug == slug).first()
    if not news: raise HTTPException(404, "Not found")
    news.views = (news.views or 0) + 1
    db.commit()
    db.refresh(news)
    return news