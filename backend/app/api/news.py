import os
import time
import json
import newspaper
import google.generativeai as genai
from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException
from sqlalchemy.orm import Session
from sqlalchemy import desc
from datetime import datetime
from apscheduler.schedulers.background import BackgroundScheduler # <--- Th∆∞ vi·ªán l√™n l·ªãch
from apscheduler.triggers.interval import IntervalTrigger     # <--- B·ªô ƒë·∫øm th·ªùi gian
from newspaper import Config
from app.core.database import get_db, SessionLocal
from app.models.news import News

router = APIRouter()

# --- 1. C·∫§U H√åNH GEMINI ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-1.5-flash')
else:
    model = None

# --- 2. H√ÄM X·ª¨ L√ù AI ---
def analyze_article_with_gemini(text, original_title):
    # (Gi·ªØ nguy√™n code x·ª≠ l√Ω AI c·ªßa b·∫°n ·ªü ƒë√¢y...)
    if not model:
        return None

    prompt = f"""
    B·∫°n l√† bi√™n t·∫≠p vi√™n c·ªßa trang tin "Z-energy". H√£y ph√¢n t√≠ch b√†i b√°o sau v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON.
    KH√îNG tr·∫£ v·ªÅ markdown (```json), ch·ªâ tr·∫£ v·ªÅ text thu·∫ßn c·ªßa JSON.
    
    Y√™u c·∫ßu output JSON ph·∫£i c√≥ c√°c tr∆∞·ªùng sau:
    1. "summary": T√≥m t·∫Øt ng·∫Øn g·ªçn (kho·∫£ng 3 c√¢u), t·∫≠p trung v√†o s·ªë li·ªáu.
    2. "category": Ch·ªçn 1 trong c√°c m·ª•c sau: ["Th·ªã tr∆∞·ªùng nƒÉng l∆∞·ª£ng", "ƒêi·ªán & H·∫° t·∫ßng", "D·∫ßu kh√≠", "NƒÉng l∆∞·ª£ng t√°i t·∫°o", "C√¥ng ngh·ªá xanh", "Ch√≠nh s√°ch"].
    3. "tags": Chu·ªói g·ªìm 3-4 t·ª´ kh√≥a, c√°ch nhau b·∫±ng d·∫•u ph·∫©y (V√≠ d·ª•: "gi√° d·∫ßu, OPEC, xƒÉng").
    4. "formatted_content": Vi·∫øt l·∫°i n·ªôi dung b√†i b√°o d∆∞·ªõi d·∫°ng Markdown. 
       - Gi·ªØ c√°c √Ω ch√≠nh v√† s·ªë li·ªáu quan tr·ªçng.
       - Chia ƒëo·∫°n r√µ r√†ng, c√≥ ti√™u ƒë·ªÅ ph·ª• (##).
       - In ƒë·∫≠m (**text**) c√°c con s·ªë ho·∫∑c t√™n ri√™ng quan tr·ªçng.
       - ƒê·ªô d√†i kho·∫£ng 400-600 t·ª´.
    
    Ti√™u ƒë·ªÅ g·ªëc: {original_title}
    N·ªôi dung b√†i b√°o:
    {text[:8000]}
    """
    
    try:
        response = model.generate_content(prompt)
        json_str = response.text.strip().replace('```json', '').replace('```', '')
        data = json.loads(json_str)
        return data
    except Exception as e:
        print(f"‚ùå L·ªói AI ho·∫∑c L·ªói JSON: {e}")
        return None

def run_crawler_process():
    """H√†m ch·∫°y ng·∫ßm: C√†o b√°o -> H·ªèi AI -> L∆∞u DB"""
    print(f"üöÄ [AUTO-CRAWL] B·∫Øt ƒë·∫ßu ti·∫øn tr√¨nh l√∫c {datetime.now()}...")
    db = SessionLocal()
    
    url = "https://cafef.vn/hang-hoa-nguyen-lieu.chn"
    
    # --- 1. C·∫§U H√åNH GI·∫¢ L·∫¨P TR√åNH DUY·ªÜT (Fix l·ªói b·ªã ch·∫∑n) ---
    config = Config()
    config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    config.request_timeout = 10
    
    try:
        # T·∫£i trang web v·ªõi c·∫•u h√¨nh gi·∫£ l·∫≠p
        paper = newspaper.build(url, config=config, memoize_articles=False)
        
        # --- DEBUG: In ra s·ªë l∆∞·ª£ng b√†i t√¨m th·∫•y ---
        print(f"üîç DEBUG: T√¨m th·∫•y t·ªïng c·ªông {len(paper.articles)} link b√†i b√°o tr√™n trang n√†y.")
        
        if len(paper.articles) == 0:
            print("‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y b√†i n√†o. C√≥ th·ªÉ trang web ƒë√£ ƒë·ªïi c·∫•u tr√∫c ho·∫∑c ch·∫∑n IP.")
            return

    except Exception as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi ƒë·∫øn b√°o: {e}")
        return

    count_new = 0
    # Th·ª≠ n·ªõi l·ªèng t·ª´ kh√≥a ƒë·ªÉ test xem c√≥ b√†i n√†o l·ªçt v√†o kh√¥ng
    keywords = ['d·∫ßu', 'xƒÉng', 'ƒëi·ªán', 'nƒÉng l∆∞·ª£ng', 'kh√≠', 'gas', 'evn', 'pin', 'xanh', 'gi√°']
    
    # TƒÉng gi·ªõi h·∫°n duy·ªát l√™n 10 b√†i ƒë·∫ßu ti√™n
    for article in paper.articles[:10]:
        try:
            article.download()
            article.parse()
            
            # Debug ti√™u ƒë·ªÅ b√†i b√°o ƒëang duy·ªát
            print(f"üëÄ ƒêang xem x√©t: {article.title}")

            # 1. L·ªçc t·ª´ kh√≥a (Check title lower)
            if not any(k in article.title.lower() for k in keywords):
                print(f"   -> B·ªè qua (Kh√¥ng ch·ª©a t·ª´ kh√≥a nƒÉng l∆∞·ª£ng)")
                continue

            # 2. Check tr√πng link g·ªëc
            if db.query(News).filter(News.original_url == article.url).first():
                print(f"   -> B·ªè qua (ƒê√£ t·ªìn t·∫°i trong DB)")
                continue

            print(f"ü§ñ ƒêang x·ª≠ l√Ω AI b√†i: {article.title[:20]}...")
            
            ai_data = analyze_article_with_gemini(article.text, article.title)
            
            # Fallback d·ªØ li·ªáu n·∫øu AI l·ªói
            summary = ai_data.get("summary", article.text[:200]) if ai_data else article.text[:200]
            content = ai_data.get("formatted_content", article.text) if ai_data else article.text
            category = ai_data.get("category", "Tin t·ª©c chung") if ai_data else "Tin t·ª©c chung"
            tags = ai_data.get("tags", "") if ai_data else ""
            
            # L·∫•y ng√†y (∆∞u ti√™n ng√†y b√°o, n·∫øu kh√¥ng c√≥ l·∫•y ng√†y gi·ªù hi·ªán t·∫°i)
            pub_date = article.publish_date if article.publish_date else datetime.now()

            new_news = News(
                title=article.title,
                slug=f"tin-{int(time.time())}-{count_new}",
                original_url=article.url,
                image_url=article.top_image,
                source="CafeF",
                published_at=pub_date,
                summary=summary,
                content=content,
                category=category,
                tags=tags,
                author="Ban bi√™n t·∫≠p",
                views=0
            )
            
            db.add(new_news)
            db.commit()
            count_new += 1
            print(f"‚úÖ ƒê√£ l∆∞u: {article.title}")
            
            time.sleep(3) # Ngh·ªâ ch√∫t
            
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói b√†i b√°o: {e}")
            continue
    
    db.close()
    print(f"üèÅ [AUTO-CRAWL] Ho√†n t·∫•t! ƒê√£ th√™m {count_new} b√†i m·ªõi.")

# --- 3. C·∫§U H√åNH SCHEDULER (T·ª∞ ƒê·ªòNG CH·∫†Y) ---
scheduler = BackgroundScheduler()

def start_scheduler():
    """H√†m n√†y s·∫Ω ƒë∆∞·ª£c g·ªçi ·ªü main.py ƒë·ªÉ k√≠ch ho·∫°t b·ªô ƒë·∫øm gi·ªù"""
    # Th√™m c√¥ng vi·ªác: Ch·∫°y h√†m run_crawler_process m·ªói 12 ti·∫øng
    scheduler.add_job(
        run_crawler_process, 
        trigger=IntervalTrigger(hours=12), 
        id='crawl_news_job', 
        replace_existing=True
    )
    scheduler.start()
    print("‚è∞ ƒê√£ k√≠ch ho·∫°t Scheduler: T·ª± ƒë·ªông c√†o tin m·ªói 12 ti·∫øng.")

# --- 4. API ENDPOINTS ---
@router.post("/crawl-now")
def trigger_crawl(background_tasks: BackgroundTasks):
    """N√∫t b·∫•m k√≠ch ho·∫°t th·ªß c√¥ng (n·∫øu kh√¥ng mu·ªën ƒë·ª£i 12 ti·∫øng)"""
    background_tasks.add_task(run_crawler_process)
    return {"message": "ƒê√£ b·∫Øt ƒë·∫ßu thu th·∫≠p tin t·ª©c. Vui l√≤ng ƒë·ª£i 1-2 ph√∫t!"}

@router.get("/")
def get_news_list(skip: int = 0, limit: int = 10, category: str = None, db: Session = Depends(get_db)):
    query = db.query(News).filter(News.is_published == True)
    if category:
        query = query.filter(News.category == category)
    news_list = query.order_by(desc(News.published_at)).offset(skip).limit(limit).all()
    return news_list

@router.get("/{slug}")
def get_news_detail(slug: str, db: Session = Depends(get_db)):
    news = db.query(News).filter(News.slug == slug).first()
    if not news:
        raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt")
    news.views += 1
    db.commit()
    return news