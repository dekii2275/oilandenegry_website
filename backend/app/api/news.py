import os
import time
import json
import hashlib
import atexit
import newspaper
import google.generativeai as genai
import concurrent.futures  # ThÆ° viá»‡n xá»­ lÃ½ Ä‘a luá»“ng
from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException
from sqlalchemy.orm import Session
from sqlalchemy import desc
from sqlalchemy.exc import IntegrityError
from datetime import datetime
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
from newspaper import Config
from tenacity import retry, stop_after_attempt, wait_exponential
from app.core.database import get_db, SessionLocal
from app.models.news import News

router = APIRouter()

# --- 1. Cáº¤U HÃŒNH GEMINI ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-2.5-flash')
else:
    model = None

# --- DANH SÃCH CATEGORY ---
ALLOWED_CATEGORIES = [
    "Thá»‹ trÆ°á»ng nÄƒng lÆ°á»£ng", "Äiá»‡n & Háº¡ táº§ng", "Dáº§u khÃ­", 
    "NÄƒng lÆ°á»£ng tÃ¡i táº¡o", "CÃ´ng nghá»‡ xanh", "ChÃ­nh sÃ¡ch", "Tin tá»©c chung"
]

# --- DANH SÃCH NGUá»’N BÃO (TrÃ¡nh bá»‹ phá»¥ thuá»™c 1 nguá»“n) ---
SOURCES = [
    "https://petrotimes.vn/nang-luong-viet-nam.html",      # ChuyÃªn ngÃ nh Dáº§u khÃ­
    "https://nangluongvietnam.vn/",                         # Táº¡p chÃ­ NÄƒng lÆ°á»£ng
    "https://vnexpress.net/tag/nang-luong-19597",           # VnExpress
    "https://congthuong.vn/nang-luong-tai-nguyen-c31",      # BÃ¡o CÃ´ng ThÆ°Æ¡ng
    "https://cafef.vn/hang-hoa-nguyen-lieu.chn"             # CafeF (Giá»¯ láº¡i lÃ m backup)
]

# --- 2. HÃ€M Xá»¬ LÃ AI ---
@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=5))
def analyze_article_with_gemini(text, original_title):
    if not model: return None
    
    prompt = f"""
    PhÃ¢n tÃ­ch bÃ i bÃ¡o nÄƒng lÆ°á»£ng sau vÃ  tráº£ vá» JSON (khÃ´ng markdown).
    YÃªu cáº§u:
    1. "summary": TÃ³m táº¯t 3 cÃ¢u cÃ³ sá»‘ liá»‡u.
    2. "category": Chá»n 1: {ALLOWED_CATEGORIES}.
    3. "tags": 3-4 tá»« khÃ³a.
    4. "formatted_content": Viáº¿t láº¡i dáº¡ng Markdown, in Ä‘áº­m sá»‘ liá»‡u, cÃ³ tiÃªu Ä‘á» phá»¥ ##.
    
    TiÃªu Ä‘á»: {original_title}
    Ná»™i dung: {text[:6000]}
    """
    try:
        response = model.generate_content(prompt)
        json_str = response.text.strip().replace('```json', '').replace('```', '')
        data = json.loads(json_str)
        if data.get("category") not in ALLOWED_CATEGORIES: data["category"] = "Tin tá»©c chung"
        return data
    except Exception:
        raise

# --- 3. HÃ€M WORKER (Xá»­ lÃ½ 1 bÃ i bÃ¡o trong luá»“ng riÃªng) ---
def process_single_article(article, source_name):
    """HÃ m nÃ y cháº¡y song song, tá»± quáº£n lÃ½ DB Session riÃªng"""
    db = SessionLocal()
    keywords = ['dáº§u', 'xÄƒng', 'Ä‘iá»‡n', 'nÄƒng lÆ°á»£ng', 'khÃ­', 'gas', 'evn', 'pvn', 'pin', 'giÃ¡', 'tháº§u']
    
    try:
        # 1. Check trÃ¹ng URL trong DB TRÆ¯á»šC khi download (Tiáº¿t kiá»‡m thá»i gian cá»±c lá»›n)
        # LÆ°u Ã½: Cáº§n check chÃ­nh xÃ¡c url hoáº·c check tÆ°Æ¡ng Ä‘á»‘i
        if db.query(News).filter(News.original_url == article.url).first():
            return 0 # Bá» qua

        # 2. Download & Parse
        try:
            article.download()
            article.parse()
        except Exception:
            return 0 

        # 3. Lá»c tá»« khÃ³a
        if not any(k in article.title.lower() for k in keywords):
            return 0

        print(f"âš¡ [Thread] Äang xá»­ lÃ½ AI: {article.title[:30]}...")

        # 4. Gá»i AI
        try:
            ai_data = analyze_article_with_gemini(article.text, article.title)
        except Exception as e:
            print(f"   âš ï¸ AI lá»—i: {e}")
            ai_data = None

        # 5. Chuáº©n bá»‹ dá»¯ liá»‡u
        summary = ai_data.get("summary", article.text[:200]) if ai_data else article.text[:200]
        content = ai_data.get("formatted_content", article.text) if ai_data else article.text
        category = ai_data.get("category", "Tin tá»©c chung") if ai_data else "Tin tá»©c chung"
        tags = ai_data.get("tags", "") if ai_data else ""
        pub_date = article.publish_date if article.publish_date else datetime.now()
        
        # Táº¡o slug unique
        url_hash = hashlib.md5(article.url.encode()).hexdigest()[:8]
        slug = f"tin-{int(time.time())}-{url_hash}"

        # 6. LÆ°u DB
        new_news = News(
            title=article.title, slug=slug, original_url=article.url,
            image_url=article.top_image, source=source_name, published_at=pub_date,
            summary=summary, content=content, category=category,
            tags=tags, author="Z-Energy Bot", views=0
        )

        try:
            db.add(new_news)
            db.commit()
            print(f"   âœ… ÄÃ£ thÃªm: {article.title[:30]}")
            return 1
        except IntegrityError:
            db.rollback()
            return 0
            
    except Exception as e:
        print(f"âŒ Lá»—i worker: {e}")
        return 0
    finally:
        db.close() # Quan trá»ng: Pháº£i Ä‘Ã³ng session cá»§a luá»“ng nÃ y

# --- 4. HÃ€M MAIN (Orchestrator) ---
def run_crawler_process():
    print(f"ğŸš€ [AUTO-CRAWL] Báº¯t Ä‘áº§u lÃºc {datetime.now()}...")
    
    config = Config()
    config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    config.request_timeout = 15
    
    total_new = 0

    # Duyá»‡t qua tá»«ng nguá»“n bÃ¡o
    for url in SOURCES:
        try:
            domain = url.split('/')[2] # Láº¥y tÃªn miá»n lÃ m source (vd: vnexpress.net)
            print(f"ğŸ“¡ Äang quÃ©t nguá»“n: {domain}...")
            
            paper = newspaper.build(url, config=config, memoize_articles=False)
            
            # Lá»c bá»›t link rÃ¡c (ngáº¯n quÃ¡ hoáº·c khÃ´ng cÃ³ http)
            valid_articles = [a for a in paper.articles if a.url and len(a.url) > 20][:6] # Láº¥y tá»‘i Ä‘a 6 bÃ i má»—i nguá»“n
            
            if not valid_articles:
                print(f"   âš ï¸ KhÃ´ng tÃ¬m tháº¥y bÃ i á»Ÿ {domain}")
                continue

            # --- CHáº Y ÄA LUá»’NG CHO NGUá»’N NÃ€Y ---
            # Max workers = 3 Ä‘á»ƒ trÃ¡nh bá»‹ cháº·n IP cá»§a tá»«ng bÃ¡o
            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                futures = {executor.submit(process_single_article, article, domain): article for article in valid_articles}
                
                for future in concurrent.futures.as_completed(futures):
                    total_new += future.result()
            
        except Exception as e:
            print(f"âŒ Lá»—i nguá»“n {url}: {e}")

    print(f"ğŸ [AUTO-CRAWL] HoÃ n táº¥t! Tá»•ng cá»™ng Ä‘Ã£ thÃªm {total_new} bÃ i má»›i.")

# --- 5. SCHEDULER & API ---
scheduler = BackgroundScheduler()

def start_scheduler():
    scheduler.add_job(run_crawler_process, trigger=IntervalTrigger(hours=12), id='crawl_news_job', replace_existing=True)
    scheduler.start()
    print("â° Scheduler activated.")
    atexit.register(lambda: scheduler.shutdown())

@router.post("/crawl-now")
def trigger_crawl(background_tasks: BackgroundTasks):
    background_tasks.add_task(run_crawler_process)
    return {"message": "Äang cháº¡y Ä‘a luá»“ng Ä‘a nguá»“n..."}

@router.get("/")
def get_news_list(skip: int = 0, limit: int = 10, category: str = None, db: Session = Depends(get_db)):
    query = db.query(News).filter(News.is_published == True)
    if category and category in ALLOWED_CATEGORIES:
        query = query.filter(News.category == category)
    return query.order_by(desc(News.published_at)).offset(skip).limit(limit).all()

@router.get("/{slug}")
def get_news_detail(slug: str, db: Session = Depends(get_db)):
    news = db.query(News).filter(News.slug == slug).first()
    if not news: raise HTTPException(404, "Not found")
    try:
        news.views += 1
        db.commit()
    except: db.rollback()
    return news