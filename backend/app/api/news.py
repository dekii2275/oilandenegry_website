import os
import time
import json
import newspaper
import google.generativeai as genai
from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException
from sqlalchemy.orm import Session
from sqlalchemy import desc
from datetime import datetime
from apscheduler.schedulers.background import BackgroundScheduler # <--- Th∆∞ vi·ªán l√™n l·ªãch
from apscheduler.triggers.interval import IntervalTrigger     # <--- B·ªô ƒë·∫øm th·ªùi gian

from app.core.database import get_db, SessionLocal
from app.models.news import News

router = APIRouter()

# --- 1. C·∫§U H√åNH GEMINI ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-1.5-flash')
else:
    model = None

# --- 2. H√ÄM X·ª¨ L√ù AI ---
def analyze_article_with_gemini(text, original_title):
    # (Gi·ªØ nguy√™n code x·ª≠ l√Ω AI c·ªßa b·∫°n ·ªü ƒë√¢y...)
    if not model:
        return None

    prompt = f"""
    B·∫°n l√† bi√™n t·∫≠p vi√™n c·ªßa trang tin "Z-energy". H√£y ph√¢n t√≠ch b√†i b√°o sau v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON.
    KH√îNG tr·∫£ v·ªÅ markdown (```json), ch·ªâ tr·∫£ v·ªÅ text thu·∫ßn c·ªßa JSON.
    
    Y√™u c·∫ßu output JSON ph·∫£i c√≥ c√°c tr∆∞·ªùng sau:
    1. "summary": T√≥m t·∫Øt ng·∫Øn g·ªçn (kho·∫£ng 3 c√¢u), t·∫≠p trung v√†o s·ªë li·ªáu.
    2. "category": Ch·ªçn 1 trong c√°c m·ª•c sau: ["Th·ªã tr∆∞·ªùng nƒÉng l∆∞·ª£ng", "ƒêi·ªán & H·∫° t·∫ßng", "D·∫ßu kh√≠", "NƒÉng l∆∞·ª£ng t√°i t·∫°o", "C√¥ng ngh·ªá xanh", "Ch√≠nh s√°ch"].
    3. "tags": Chu·ªói g·ªìm 3-4 t·ª´ kh√≥a, c√°ch nhau b·∫±ng d·∫•u ph·∫©y (V√≠ d·ª•: "gi√° d·∫ßu, OPEC, xƒÉng").
    4. "formatted_content": Vi·∫øt l·∫°i n·ªôi dung b√†i b√°o d∆∞·ªõi d·∫°ng Markdown. 
       - Gi·ªØ c√°c √Ω ch√≠nh v√† s·ªë li·ªáu quan tr·ªçng.
       - Chia ƒëo·∫°n r√µ r√†ng, c√≥ ti√™u ƒë·ªÅ ph·ª• (##).
       - In ƒë·∫≠m (**text**) c√°c con s·ªë ho·∫∑c t√™n ri√™ng quan tr·ªçng.
       - ƒê·ªô d√†i kho·∫£ng 400-600 t·ª´.
    
    Ti√™u ƒë·ªÅ g·ªëc: {original_title}
    N·ªôi dung b√†i b√°o:
    {text[:8000]}
    """
    
    try:
        response = model.generate_content(prompt)
        json_str = response.text.strip().replace('```json', '').replace('```', '')
        data = json.loads(json_str)
        return data
    except Exception as e:
        print(f"‚ùå L·ªói AI ho·∫∑c L·ªói JSON: {e}")
        return None

def run_crawler_process():
    """H√†m ch·∫°y ng·∫ßm: C√†o b√°o -> H·ªèi AI -> L∆∞u DB"""
    print(f"üöÄ [AUTO-CRAWL] B·∫Øt ƒë·∫ßu ti·∫øn tr√¨nh l√∫c {datetime.now()}...")
    db = SessionLocal()
    
    url = "https://cafef.vn/hang-hoa-nguyen-lieu.chn"
    paper = newspaper.build(url, memoize_articles=False)
    
    count_new = 0
    keywords = ['d·∫ßu', 'xƒÉng', 'ƒëi·ªán', 'nƒÉng l∆∞·ª£ng', 'kh√≠', 'gas', 'evn', 'pin', 'xanh']
    
    # L·∫•y 5 b√†i m·ªói l·∫ßn ch·∫°y
    for article in paper.articles[:5]:
        try:
            article.download()
            article.parse()
            
            if not any(k in article.title.lower() for k in keywords):
                continue

            if db.query(News).filter(News.original_url == article.url).first():
                continue

            print(f"ü§ñ ƒêang x·ª≠ l√Ω AI b√†i: {article.title[:20]}...")
            
            ai_data = analyze_article_with_gemini(article.text, article.title)
            
            summary = ai_data.get("summary", article.text[:200]) if ai_data else article.text[:200]
            content = ai_data.get("formatted_content", article.text) if ai_data else article.text
            category = ai_data.get("category", "Tin t·ª©c chung") if ai_data else "Tin t·ª©c chung"
            tags = ai_data.get("tags", "") if ai_data else ""

            pub_date = article.publish_date if article.publish_date else datetime.now()

            new_news = News(
                title=article.title,
                slug=f"tin-{int(time.time())}-{count_new}",
                original_url=article.url,
                image_url=article.top_image,
                source="CafeF",
                published_at=pub_date,
                summary=summary,
                content=content,
                category=category,
                tags=tags,
                author="Ban bi√™n t·∫≠p",
                views=0
            )
            
            db.add(new_news)
            db.commit()
            count_new += 1
            print(f"‚úÖ ƒê√£ l∆∞u: {article.title}")
            time.sleep(3)
            
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói b√†i b√°o: {e}")
            continue
    
    db.close()
    print(f"üèÅ [AUTO-CRAWL] Ho√†n t·∫•t! ƒê√£ th√™m {count_new} b√†i m·ªõi.")

# --- 3. C·∫§U H√åNH SCHEDULER (T·ª∞ ƒê·ªòNG CH·∫†Y) ---
scheduler = BackgroundScheduler()

def start_scheduler():
    """H√†m n√†y s·∫Ω ƒë∆∞·ª£c g·ªçi ·ªü main.py ƒë·ªÉ k√≠ch ho·∫°t b·ªô ƒë·∫øm gi·ªù"""
    # Th√™m c√¥ng vi·ªác: Ch·∫°y h√†m run_crawler_process m·ªói 12 ti·∫øng
    scheduler.add_job(
        run_crawler_process, 
        trigger=IntervalTrigger(hours=12), 
        id='crawl_news_job', 
        replace_existing=True
    )
    scheduler.start()
    print("‚è∞ ƒê√£ k√≠ch ho·∫°t Scheduler: T·ª± ƒë·ªông c√†o tin m·ªói 12 ti·∫øng.")

# --- 4. API ENDPOINTS ---
@router.post("/crawl-now")
def trigger_crawl(background_tasks: BackgroundTasks):
    """N√∫t b·∫•m k√≠ch ho·∫°t th·ªß c√¥ng (n·∫øu kh√¥ng mu·ªën ƒë·ª£i 12 ti·∫øng)"""
    background_tasks.add_task(run_crawler_process)
    return {"message": "ƒê√£ b·∫Øt ƒë·∫ßu thu th·∫≠p tin t·ª©c. Vui l√≤ng ƒë·ª£i 1-2 ph√∫t!"}

@router.get("/")
def get_news_list(skip: int = 0, limit: int = 10, category: str = None, db: Session = Depends(get_db)):
    query = db.query(News).filter(News.is_published == True)
    if category:
        query = query.filter(News.category == category)
    news_list = query.order_by(desc(News.published_at)).offset(skip).limit(limit).all()
    return news_list

@router.get("/{slug}")
def get_news_detail(slug: str, db: Session = Depends(get_db)):
    news = db.query(News).filter(News.slug == slug).first()
    if not news:
        raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt")
    news.views += 1
    db.commit()
    return news